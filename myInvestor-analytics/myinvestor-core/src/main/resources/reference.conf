myInvestor {
  app-name = "myInvestor"
}

spark {

  spark.checkpoint.dir = "./tmp"

  # The Spark master host. Set first by environment if exists. Then system, then config.
  # Options: spark://host1:port1,host2:port2
  # - "local" to run locally with one thread,
  # - local[4]" to run locally with 4 cores
  # - the master IP/hostname to run on a Spark standalone cluster
  # - if not set, defaults to "local[*]" to run with enough threads
  # Supports optional HA failover for more than one: host1,host2..
  # which is used to inform: spark://host1:port1,host2:port2
  master = ${?SPARK_HA_MASTER}
  cleaner.ttl = ${?SPARK_CLEANER_TTL}

  # The batch interval must be set based on the latency requirements
  # of your application and available cluster resources.
  streaming.batch.interval = ${?SPARK_STREAMING_BATCH_INTERVAL}
}

kafka {
  hosts = [${?KAFKA_HOSTS}]
  # If not found in the env, defaults to
  hosts = ["127.0.0.1:9092"]
  ingest-rate = 1s
  group.id = "myinvestor.group"
  topic.exchange = "myinvestor.exchange"
  deserializer.fqcn = "org.apache.kafka.common.serialization.StringDeserializer"
  auto-offset-reset = "latest"
  enable.auto.commit = "false"
  batch.send.size = 100
}

cassandra {
  # The contact point to connect to the Cassandra cluster.
  # Accepts a comma-separated string of hosts. Override with -Dcassandra.connection.host.
  connection.host = ${?CASSANDRA_SEEDS}

  # Cassandra thrift port. Defaults to 9160. Override with -Dcassandra.connection.rpc.port.
  connection.rpc.port = ${?CASSANDRA_RPC_PORT}

  # Cassandra native port. Defaults to 9042. Override with -Dcassandra.connection.native.port.
  connection.native.port = ${?CASSANDRA_NATIVE_PORT}

  # Auth: These are expected to be set in the env by chef, etc.
  # The username for authentication. Override with -Dcassandra.auth.username.
  auth.username = ${?CASSANDRA_AUTH_USERNAME}
  # The password for authentication. Override with -Dcassandra.auth.password.
  auth.password = ${?CASSANDRA_AUTH_PASSWORD}

  ## Tuning ##
}

akka {
  loglevel = "DEBUG"
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  logger-startup-timeout = 60s
  log-dead-letters = off
  log-dead-letters-during-shutdown = off

  remote {
    log-remote-lifecycle-events = off
    netty.tcp {
      port = 2550
      hostname = "127.0.0.1"
    }
  }

  actor {
    provider = "akka.cluster.ClusterActorRefProvider"
    default-dispatcher {
      # Throughput for default Dispatcher, set to 1 for as fair as possible
      throughput = 10
    }
  }

  cluster {
    log-info = on
    seed-nodes = []
    gossip-interval = 5s
    publish-stats-interval = 10s
    #auto-down-unreachable-after = 10s
    metrics.gossip-interval = 10s
    metrics.collect-interval = 10s
  }
}